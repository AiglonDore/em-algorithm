---
title: 'Pratical work: EM algorithm'
author: "Houda Aiboud Benchekroun and Thomas Roiseux"
date: "`r Sys.Date()`"
output: pdf_document
header-includes: \DeclareMathOperator{\E}{\mathbb{E}}
---
\part{Simulation}
We first generate:
\begin{enumerate}
\item A sample of \(n=100\) observations with a Poisson law using \(\lambda=3\).
\item A sample of \(n=200\) observations with a Poisson law using \(\lambda=15\).
\item A vector of \(300\) coordinates, which the 100 first are 1 and the others are 2.
\end{enumerate}
```{r generatingPoisson}
sample3 <- rpois(100, 3)
print(sample3)
sample15 <- rpois(200, 15)
print(sample15)
v <- c()
for (i in 1:100)
{
  v <- c(v, 1, recursive = TRUE)
}
for (i in 1:200)
{
  v <- c(v, 2, recursive = TRUE)
}

```
Now, we are going to generate a Poisson law using two components:
```{r poisson2Comp}
#Settings constants
pi1 <- 0.4
pi2 <- 0.6
lambda1 <- 3
lambda2 <- 15
sample1 <- rpois(300, lambda1)
sample2 <- rpois(300,lambda2)


#on complète avec les z cachées 

Z<-rbinom(300,1,0.4)#les zi suivent une loi de bernouilli de parametre 0.4 
                    #cad la probabilité que xi soit dans ce groupe
mixed_sample=c()
for (i in 1:300){
  if (Z[i]==1){
    mixed_sample=c(mixed_sample,sample1[i])
  }
  else{
    mixed_sample=c(mixed_sample,sample2[i])
  }
}
print(mixed_sample)
```

\clearpage
\part{EM-algorithm}
The EM-algorithm is a recursive algorithm used to maximize the log-likelihood on incomplete data.
Therefore, it requires to impute the missing elements, with a probability
\(f(z_i\mid x_i,\theta)\), \(z_i\) the missing element.
\par
We now define the completed log-likelihood:
\[L((x,z),\theta)=\sum_{i=1}^{n}\ln(f(z_i\mid x_i,\theta))-\ln(f(x_i,\theta))\]
We now denote \(\theta^{(c)}\) the indicator of the parameters and, as \(L(x,z),\theta)\) is independent with \(z\),
we finally have \(\E[L(x,\theta)\mid\theta^{(c)}]=L((x,z),\theta)\).
Therefore:
\[L((x,z),\theta)=\underbrace{\E[L((x,z),\theta)\mid \theta^{(c)}]}_{Q(\theta,\theta^{(c)})}
-\underbrace{\E\left[\sum_{i=1}^{n}\ln(f(z_i\mid x_i,\theta))\middle|\theta^{(c)}\right]}_{H(\theta,\theta^{(c)})}
\]
To maximize the log-likelihood, we recursively look for the best \(\theta^{(c)}\), that maximizes \(Q(\theta,\theta^{(c)})\).\par
In this case, we replace \(f\) with the given density.
\par
The algorithm works as following:
\begin{enumerate}
\item Beginning in a random position \(\theta^{(0)}\).
\item Computing expectancy with \(Q(\theta,\theta^{(c)})\).
\item Calculating \(\theta^{(c+1)}\) to maximize the expectancy.
\item Redoing the 2 previous steps until convergence.
\end{enumerate}
\section{Initialization}

```{r}

initialiser<-function(k){
  teta<-c()
  for (i in 1:k){
    teta[i]=1/k
  }
  for (j in (k+1):(k+k)){
    teta[j]=floor(runif(n=1,1,15))
  }
  return(teta)
}



```

\section{Step E}
We denote \(\Phi=(\pi_1,\ldots,\pi_k,\\theta_1,\ldots,\theta_k)\) and then we have:
\[
t_{i,k}=\E[z_{i,k}\mid x,\Phi^{(c)}]=\frac{\pi_k^{(c)}f(x_i,\theta_k^{(c)})}{\sum_{l=1}^q\pi_l^{(c)}f(x_i\theta_l^{(c)})}
\]
with \(z\) the hidden vector which represents \(1_{i=1}\).
```{r}
f<-function(X,teta,k,i){
  tmp<-0
  for (l in 1:k){
        tmp=tmp + (teta[l]*dpois(X[i],teta[k+l]))
  }
  return(tmp)
  
}

step_E <- function(X, teta, k){
  T<-matrix(nrow = length(X), ncol = k)
  
  for (i in 1:length(X)){
    tmp = 0
    for (l in 1:k){
      tmp = tmp + teta[l] * dpois(X[i], teta[k + l])
    }
    for (j in 1:k){
      T[i, j] = (teta[j] * dpois(X[i], teta[k + j]))/tmp
    }
  }
  return(T)
}
```


\section{Step M}
We finally have:
\[\theta^{(k+1)}=\frac{\sum_{i=1}^nt_{i,k}x_i}{\sum_{i=1}^nt_{i,k}}\]
And:
\[\pi_k=\frac{\sum_{i=1}^{n}t_{i,k}}{n}\]
```{r}

step_M<-function(X,t,K){
  teta_opt<-c()
   for (k in 1:K){
    tmp1 = 0
    tmp2 = 0
    for (i in 1:length(X)){
      tmp1 =tmp1+t[i, k]*X[i]
    }
    
    for (i in 1:length(X)){
      tmp2 = tmp2 + t[i, k]
     }
  
    
    teta_opt[k] = (1/length(X)) * tmp2
    teta_opt[K + k] = tmp1/tmp2
   }
  return(teta_opt)
}
```

\section{Complete algorithm}

```{r}
#on va définir la norme 2 pour qu'on puisse calculer le critère de convergence

norme_2<-function(a){
  res=0
  for (i in 1:length(a)){
    res=res+(a[i]**2)
  }
  return(res**0.5)
}

#l'algo

Em_algo <- function(X, K){
  teta = initialiser(K)
  t = step_E(X,teta,K)
  teta_opt = step_M(X,t,K)
  while((((norme_2(teta-teta_opt))**2)/(norme_2(teta)**2))>10**(-6)){ #critère de convergence
    teta=teta_opt
    t = step_E(X,teta_opt,K)
    teta_opt = step_M(X,t,K)
    
  }
  return(teta_opt)
}


Em_algo(mixed_sample, 2)




```













